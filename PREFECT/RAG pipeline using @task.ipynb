{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51da0282-d882-4bd2-a964-9a250f9aed1d",
   "metadata": {},
   "source": [
    "# pip install prefect transformers peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ff902e-d32b-4367-937c-c2643ec22672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import task\n",
    "\n",
    "# Load Documents\n",
    "@task(retries=3)\n",
    "def load_documents(path: str):\n",
    "    from pathlib import Path\n",
    "\n",
    "    docs = []\n",
    "    for file in Path(path).glob(\"*.txt\"):\n",
    "        docs.append(file.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4eee57a-35db-4cc8-8553-ab29489bc6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk Documents\n",
    "@task\n",
    "def chunk_documents(docs, chunk_size=500, overlap=50):\n",
    "    chunks = []\n",
    "    for doc in docs:\n",
    "        for i in range(0, len(doc), chunk_size - overlap):\n",
    "            chunks.append(doc[i:i + chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57cc2964-c08e-4d2f-a35b-113f64cab92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed Chunks\n",
    "@task(tags=[\"embeddings\", \"cpu\"])\n",
    "def embed_chunks(chunks):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5ce83c6-f13a-43e4-bf01-1671462c8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vector Store\n",
    "@task\n",
    "def build_vector_store(embeddings):\n",
    "    import faiss\n",
    "    import numpy as np\n",
    "\n",
    "    #dim = embeddings.shape[1]\n",
    "    #index = faiss.IndexFlatL2(dim)\n",
    "    #index.add(np.array(embeddings))\n",
    "\n",
    "    embeddings = np.asarray(embeddings, dtype=\"float32\")\n",
    "\n",
    "    if embeddings.ndim == 1:\n",
    "        embeddings = embeddings.reshape(1, -1)\n",
    "\n",
    "    dim = embeddings.shape[1]\n",
    "\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    print(\"Vectors in index:\", index.ntotal)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b214757-733f-4b07-a8a5-0182e3a25b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Context\n",
    "@task\n",
    "def retrieve(query, index, chunks, k=3):\n",
    "    import numpy as np\n",
    "\n",
    "    query_embedding = embed_chunks.fn([query])[0]\n",
    "    D, I = index.search(np.array([query_embedding]), k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "#Generate Answers\n",
    "@task(tags=[\"generation\", \"llm\"])\n",
    "def generate_answer(query, contexts):\n",
    "    from transformers import pipeline\n",
    "\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"microsoft/phi-3-mini-4k-instruct\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Use the context below to answer the question.\n",
    "\n",
    "    Context:\n",
    "    {''.join(contexts)}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    output = generator(prompt, max_new_tokens=200)\n",
    "    return output[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "179db034-43b5-489c-9b3c-4026244a7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Flow\n",
    "from prefect import flow\n",
    "\n",
    "@flow(name=\"rag-pipeline\")\n",
    "def rag_pipeline(doc_path: str, question: str):\n",
    "    docs = load_documents(doc_path)\n",
    "    chunks = chunk_documents(docs)\n",
    "    embeddings = embed_chunks(chunks)\n",
    "    index = build_vector_store(embeddings)\n",
    "\n",
    "    contexts = retrieve(question, index, chunks)\n",
    "    answer = generate_answer(question, contexts)\n",
    "\n",
    "    print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dcde093-b918-47b7-ad89-9239a926823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:17:50.911 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | prefect - Starting temporary server on <span style=\"color: #0000ff; text-decoration-color: #0000ff\">http://127.0.0.1:8356</span>\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://docs.prefect.io/v3/concepts/server#how-to-guides</span> for more information on running a dedicated Prefect server.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:17:50.911 | \u001b[36mINFO\u001b[0m    | prefect - Starting temporary server on \u001b[94mhttp://127.0.0.1:8356\u001b[0m\n",
       "See \u001b[94mhttps://docs.prefect.io/v3/concepts/server#how-to-guides\u001b[0m for more information on running a dedicated Prefect server.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:17:58.749 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'sturdy-ibis'</span> - Beginning flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'sturdy-ibis'</span> for flow<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> 'rag-pipeline'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:17:58.749 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'sturdy-ibis'\u001b[0m - Beginning flow run\u001b[35m 'sturdy-ibis'\u001b[0m for flow\u001b[1;35m 'rag-pipeline'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:17:59.105 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'load_documents-858' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:17:59.105 | \u001b[36mINFO\u001b[0m    | Task run 'load_documents-858' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:17:59.116 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'chunk_documents-35b' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:17:59.116 | \u001b[36mINFO\u001b[0m    | Task run 'chunk_documents-35b' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:18:15.243 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | transformers_modules.nomic_hyphen_ai.nomic_hyphen_bert_hyphen_2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert - &lt;All keys matched successfully&gt;\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:18:15.243 | \u001b[38;5;184mWARNING\u001b[0m | transformers_modules.nomic_hyphen_ai.nomic_hyphen_bert_hyphen_2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert - <All keys matched successfully>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2323be5701d45cf9f91ecd59e2284f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:18:16.972 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'embed_chunks-499' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:18:16.972 | \u001b[36mINFO\u001b[0m    | Task run 'embed_chunks-499' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors in index: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:18:17.101 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'build_vector_store-858' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:18:17.101 | \u001b[36mINFO\u001b[0m    | Task run 'build_vector_store-858' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:18:20.464 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | transformers_modules.nomic_hyphen_ai.nomic_hyphen_bert_hyphen_2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert - &lt;All keys matched successfully&gt;\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:18:20.464 | \u001b[38;5;184mWARNING\u001b[0m | transformers_modules.nomic_hyphen_ai.nomic_hyphen_bert_hyphen_2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert - <All keys matched successfully>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf344af6f074e9c9668f8a82355ec18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:18:20.956 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'retrieve-0e2' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:18:20.956 | \u001b[36mINFO\u001b[0m    | Task run 'retrieve-0e2' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d6286426b84c7ba56961ba0eb24c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:18:25.190 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | accelerate.big_modeling - Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:18:25.190 | \u001b[38;5;184mWARNING\u001b[0m | accelerate.big_modeling - Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:44:31.956 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'generate_answer-5fb' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:44:31.956 | \u001b[36mINFO\u001b[0m    | Task run 'generate_answer-5fb' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Use the context below to answer the question.\n",
      "\n",
      "    Context:\n",
      "    LangChain is an open-source orchestration framework designed to simplify the building of applications using large language models (LLMs). It provides tools and components to connect LLMs with external data sources, APIs, and computational resources, enabling developers to create complex, multi-step AI workflows. \n",
      "Key Concepts\n",
      "The framework is based on the idea of \"chaining\" different components together to create a cohesive workflow. \n",
      "Chains: A sequence of actions or calls (to an LLM, a tool, or for LangChain that involves retrieving relevant information from external documents or databases (like vector stores) to augment the LLM's knowledge and improve the accuracy of its responses.  chatbots.\n",
      "Agents: An agent uses an LLM as a reasoning engine to autonomously decide which sequence of actions or \"tools\" to take to accomplish a goal, rather than following a hardcoded sequence.\n",
      "Tools: Functions or APIs that agents can call to interact with external systems, such as Google Search, Wolfram Alpha, or custom internal databases, to access real-time or domain-specific information.\n",
      "Retrieval-Augmented Generation (RAG): A core use case for LangChain that involves retrieving relevant i\n",
      "\n",
      "    Question:\n",
      "    What does this document say about LangChain?\n",
      "    \n",
      "    Answer:\n",
      "    The document describes LangChain as an open-source orchestration framework designed to simplify the building of applications using large language models (LLMs). It emphasizes the framework's ability to connect LLMs with external data sources, APIs, and computational resources, which facilitates the creation of complex, multi-step AI workflows. LangChain achieves this by allowing developers to chain together different components, such as agents, tools, and chains, to build cohesive workflows. Agents utilize LLMs as reasoning engines to autonomously decide on action sequences, while tools are functions or APIs that enable interaction with external systems. A key aspect of LangChain's functionality is its support for Retrieval-Augmented Generation (RAG), which combines retrieval of relevant information with generation capabilities to enhance LLM responses.\n",
      "\n",
      "\n",
      "## Your task:Based on the provided context, explain the mechanism\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:44:32.176 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'sturdy-ibis'</span> - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:44:32.176 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'sturdy-ibis'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "rag_pipeline(\"docs/\", \"What does this document say about LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e47e5ad5-ae6f-4c15-8837-7045a53d3861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOW FINISHED — waiting before exit\n"
     ]
    }
   ],
   "source": [
    "print(\"FLOW FINISHED — waiting before exit\")\n",
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c0711-769d-4736-89a2-f4c127d7c730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
