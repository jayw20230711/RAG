{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8c3ecb-7c0d-4955-91d6-6cbd071a0c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#!pip install openai anthropic google-generativeai mistralai python-dotenv\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AI Models Python Notebook Setup Guide\n",
    "# Installation & Imports\n",
    "\"\"\"\n",
    "#!pip install openai anthropic google-generativeai mistralai python-dotenv\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df57c34b-c30e-4304-8d27-8ea3a19b8642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import time  \n",
    "from typing import Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "from mistralai import Mistral\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe28daf-8cf4-463c-b0c9-23643f369ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 1. OpenAI Setup ====================\n",
    "class OpenAIClient:\n",
    "    def __init__(self, model: str = \"gpt-4o\"):  # Updated to current model\n",
    "        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.async_client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.model = model\n",
    "        \n",
    "    def chat_completion(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Basic chat completion with error handling\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                **kwargs\n",
    "            )\n",
    "            return {\n",
    "                \"content\": response.choices[0].message.content,\n",
    "                \"usage\": {\n",
    "                    \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                    \"completion_tokens\": response.usage.completion_tokens,\n",
    "                    \"total_tokens\": response.usage.total_tokens\n",
    "                },\n",
    "                \"model\": response.model\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return self._handle_error(e)\n",
    "    \n",
    "    async def async_chat_completion(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Async version for multiple requests\"\"\"\n",
    "        try:\n",
    "            response = await self.async_client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                **kwargs\n",
    "            )\n",
    "            return {\n",
    "                \"content\": response.choices[0].message.content,\n",
    "                \"usage\": {\n",
    "                    \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                    \"completion_tokens\": response.usage.completion_tokens,\n",
    "                    \"total_tokens\": response.usage.total_tokens\n",
    "                },\n",
    "                \"model\": response.model\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return self._handle_error(e)\n",
    "    \n",
    "    def code_generation(self, prompt: str, language: str = \"python\") -> Dict[str, Any]:\n",
    "        \"\"\"Specialized method for code generation\"\"\"\n",
    "        system_message = f\"You are an expert {language} programmer. Write clean, efficient code with proper error handling.\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        return self.chat_completion(\n",
    "            messages, \n",
    "            temperature=0.2,\n",
    "            max_tokens=2000\n",
    "        )\n",
    "    \n",
    "    def _handle_error(self, error: Exception) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive error handling\"\"\"\n",
    "        error_type = type(error).__name__\n",
    "        error_mapping = {\n",
    "            \"AuthenticationError\": \"Invalid API key\",\n",
    "            \"RateLimitError\": \"Rate limit exceeded\",\n",
    "            \"APIError\": \"OpenAI API error\",\n",
    "            \"APITimeoutError\": \"Request timeout\"  # FIXED: Correct error name\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"error\": True,\n",
    "            \"message\": error_mapping.get(error_type, str(error)),\n",
    "            \"type\": error_type\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3488f10e-1fb4-4a3d-9c44-264ade6f3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 2. Google Gemini Setup ====================\n",
    "class GeminiClient:\n",
    "    def __init__(self, model: str = \"gemini-2.0-flash-exp\"):  # Updated model name\n",
    "        genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "        self.model = genai.GenerativeModel(model)\n",
    "        self.model_name = model\n",
    "        \n",
    "    def generate_content(self, prompt: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Generate content with safety settings\"\"\"\n",
    "        try:\n",
    "            generation_config = genai.types.GenerationConfig(\n",
    "                temperature=kwargs.get('temperature', 0.1),\n",
    "                max_output_tokens=kwargs.get('max_tokens', 2000),\n",
    "                top_p=kwargs.get('top_p', 0.95)  # FIXED: Made configurable\n",
    "            )\n",
    "            \n",
    "            safety_settings = [\n",
    "                {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "                {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "                {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "                {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"}\n",
    "            ]\n",
    "            \n",
    "            response = self.model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=generation_config,\n",
    "                safety_settings=safety_settings\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"content\": response.text,\n",
    "                \"safety_ratings\": self._parse_safety_ratings(response),\n",
    "                \"model\": self.model_name\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return self._handle_error(e)\n",
    "    \n",
    "    def chat_session(self, history: List[Dict] = None):\n",
    "        \"\"\"Start a chat session for back-and-forth programming\"\"\"\n",
    "        try:\n",
    "            chat = self.model.start_chat(history=history or [])\n",
    "            return chat\n",
    "        except Exception as e:\n",
    "            print(f\"Chat session error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def code_analysis(self, code: str, language: str = \"python\") -> Dict[str, Any]:\n",
    "        \"\"\"Analyze code for bugs and improvements\"\"\"\n",
    "        prompt = f\"\"\"Analyze this {language} code and provide:\n",
    "                    1. Potential bugs or issues\n",
    "                    2. Performance improvements\n",
    "                    3. Code style suggestions\n",
    "                    4. Security concerns\n",
    "\n",
    "                    Code:\n",
    "                    ```{language}\n",
    "                    {code}\n",
    "                    ```\n",
    "                    \"\"\"\n",
    "        return self.generate_content(prompt, temperature=0.1)\n",
    "    \n",
    "    def _parse_safety_ratings(self, response) -> List[Dict]:\n",
    "        \"\"\"Parse safety ratings from response\"\"\"\n",
    "        safety_ratings = []\n",
    "        try:\n",
    "            if hasattr(response, 'candidates') and response.candidates:\n",
    "                for rating in response.candidates[0].safety_ratings:\n",
    "                    safety_ratings.append({\n",
    "                        \"category\": rating.category.name,\n",
    "                        \"probability\": rating.probability.name\n",
    "                    })\n",
    "        except Exception:\n",
    "            pass\n",
    "        return safety_ratings\n",
    "    \n",
    "    def _handle_error(self, error: Exception) -> Dict[str, Any]:\n",
    "        error_type = type(error).__name__\n",
    "        return {\n",
    "            \"error\": True,\n",
    "            \"message\": str(error),\n",
    "            \"type\": error_type\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f24b181c-397f-4d85-9b9f-a34eb2beb197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 3. Anthropic Claude Setup ====================\n",
    "class ClaudeClient:\n",
    "    def __init__(self, model: str = \"claude-sonnet-4-20250514\"):  \n",
    "        self.client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "        self.model = model\n",
    "        self.max_tokens = 4000\n",
    "        \n",
    "    def message(self, prompt: str, system: str = None, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Send message to Claude with system prompt\"\"\"\n",
    "        try:\n",
    "            message_params = {\n",
    "                \"model\": self.model,\n",
    "                \"max_tokens\": kwargs.get('max_tokens', self.max_tokens),\n",
    "                \"temperature\": kwargs.get('temperature', 0.1),\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "            }\n",
    "            \n",
    "            if system:\n",
    "                message_params[\"system\"] = system\n",
    "            \n",
    "            message = self.client.messages.create(**message_params)\n",
    "            \n",
    "            return {\n",
    "                \"content\": message.content[0].text,\n",
    "                \"usage\": {\n",
    "                    \"input_tokens\": message.usage.input_tokens,\n",
    "                    \"output_tokens\": message.usage.output_tokens\n",
    "                },\n",
    "                \"model\": message.model,\n",
    "                \"stop_reason\": message.stop_reason\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return self._handle_error(e)\n",
    "    \n",
    "    def programming_assistant(self, task: str, code_context: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Specialized method for programming tasks\"\"\"\n",
    "        system_prompt = \"\"\"You are an expert programming assistant. Follow these rules:\n",
    "1. Write clean, efficient, and well-documented code\n",
    "2. Include error handling and edge cases\n",
    "3. Provide explanations for complex logic\n",
    "4. Suggest improvements and alternatives\"\"\"\n",
    "        \n",
    "        if code_context:\n",
    "            prompt = f\"Code context:\\n{code_context}\\n\\nTask: {task}\"\n",
    "        else:\n",
    "            prompt = task\n",
    "            \n",
    "        return self.message(prompt, system=system_prompt, temperature=0.2)\n",
    "    \n",
    "    def stream_response(self, prompt: str, system: str = None):\n",
    "        \"\"\"Stream response for long-running tasks\"\"\"\n",
    "        try:\n",
    "            with self.client.messages.stream(\n",
    "                model=self.model,\n",
    "                max_tokens=self.max_tokens,\n",
    "                temperature=0.1,\n",
    "                system=system,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            ) as stream:\n",
    "                for text in stream.text_stream:\n",
    "                    yield text\n",
    "                    \n",
    "        except Exception as e:\n",
    "            yield f\"Error: {str(e)}\"\n",
    "    \n",
    "    def _handle_error(self, error: Exception) -> Dict[str, Any]:\n",
    "        error_type = type(error).__name__\n",
    "        error_mapping = {\n",
    "            \"AuthenticationError\": \"Invalid API key\",\n",
    "            \"RateLimitError\": \"Rate limit exceeded\",\n",
    "            \"APIError\": \"Anthropic API error\",\n",
    "            \"OverloadedError\": \"API overloaded\"\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"error\": True,\n",
    "            \"message\": error_mapping.get(error_type, str(error)),\n",
    "            \"type\": error_type\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce4005db-b58e-44ff-84d5-e49d55dd1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 4. DeepSeek Setup ====================\n",
    "class DeepSeekClient:\n",
    "    def __init__(self, model: str = \"deepseek-chat\"):  \n",
    "        self.client = OpenAI(\n",
    "            api_key=os.getenv('DEEPSEEK_API_KEY'),\n",
    "            base_url=\"https://api.deepseek.com\"  \n",
    "        )\n",
    "        self.model = model\n",
    "        \n",
    "    def chat(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Chat completion with DeepSeek\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"content\": response.choices[0].message.content,\n",
    "                \"usage\": {\n",
    "                    \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                    \"completion_tokens\": response.usage.completion_tokens,\n",
    "                    \"total_tokens\": response.usage.total_tokens\n",
    "                },\n",
    "                \"model\": response.model\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return self._handle_error(e)\n",
    "    \n",
    "    def generate_code(self, requirement: str, language: str = \"python\") -> Dict[str, Any]:\n",
    "        \"\"\"Generate code with specific requirements\"\"\"\n",
    "        system_msg = f\"You are an expert {language} programmer. Write efficient, production-ready code.\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": f\"Write {language} code for: {requirement}\"}\n",
    "        ]\n",
    "        \n",
    "        return self.chat(\n",
    "            messages,\n",
    "            temperature=0.1,\n",
    "            max_tokens=4000\n",
    "        )\n",
    "    \n",
    "    def code_review(self, code: str, language: str = \"python\") -> Dict[str, Any]:\n",
    "        \"\"\"Perform code review\"\"\"\n",
    "        prompt = f\"\"\"Review this {language} code and provide:\n",
    "\n",
    "CRITICAL:\n",
    "- Security vulnerabilities\n",
    "- Major bugs\n",
    "- Performance issues\n",
    "\n",
    "IMPROVEMENTS:\n",
    "- Code style\n",
    "- Best practices\n",
    "- Documentation\n",
    "\n",
    "Code:\n",
    "```{language}\n",
    "{code}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        return self.chat(messages, temperature=0.1)\n",
    "    \n",
    "    def _handle_error(self, error: Exception) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"error\": True,\n",
    "            \"message\": str(error),\n",
    "            \"type\": type(error).__name__\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48ba8e3-bb05-4aa6-a313-3283e571daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 5. Mistral AI Setup ====================\n",
    "class MistralClient:\n",
    "    def __init__(self, model: str = \"mistral-large-latest\"):  \n",
    "        self.client = Mistral(api_key=os.getenv('MISTRAL_API_KEY'))\n",
    "        self.model = model\n",
    "        \n",
    "    def chat(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Chat completion with Mistral\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.complete(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"content\": response.choices[0].message.content,\n",
    "                \"usage\": {\n",
    "                    \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                    \"completion_tokens\": response.usage.completion_tokens,\n",
    "                    \"total_tokens\": response.usage.total_tokens\n",
    "                },\n",
    "                \"model\": response.model\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return self._handle_error(e)\n",
    "    \n",
    "    async def async_chat(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Async chat completion\"\"\"\n",
    "        try:\n",
    "            # Note: Mistral async requires separate async client\n",
    "            response = await asyncio.to_thread(\n",
    "                self.client.chat.complete,\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"content\": response.choices[0].message.content,\n",
    "                \"usage\": {\n",
    "                    \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                    \"completion_tokens\": response.usage.completion_tokens,\n",
    "                    \"total_tokens\": response.usage.total_tokens\n",
    "                },\n",
    "                \"model\": response.model\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return self._handle_error(e)\n",
    "    \n",
    "    def multi_language_code(self, requirement: str, languages: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate code in multiple languages\"\"\"\n",
    "        prompt = f\"\"\"Create implementation for: {requirement}\n",
    "\n",
    "Provide code in these languages: {', '.join(languages)}\n",
    "Include proper error handling and documentation for each.\n",
    "\"\"\"\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        return self.chat(messages, max_tokens=6000)\n",
    "    \n",
    "    def _handle_error(self, error: Exception) -> Dict[str, Any]:\n",
    "        error_type = type(error).__name__\n",
    "        error_mapping = {\n",
    "            \"APIConnectionError\": \"Connection error\",\n",
    "            \"RateLimitError\": \"Rate limit exceeded\",\n",
    "            \"APIError\": \"Mistral API error\"\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"error\": True,\n",
    "            \"message\": error_mapping.get(error_type, str(error)),\n",
    "            \"type\": error_type\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1415f400-8e55-4f81-96cc-dcbef49d8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 6. Universal Client Manager ====================\n",
    "class AIModelManager:\n",
    "    \"\"\"Universal manager for all AI models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clients = {\n",
    "            \"openai\": OpenAIClient(),\n",
    "            \"gemini\": GeminiClient(),\n",
    "            \"claude\": ClaudeClient(),\n",
    "            \"deepseek\": DeepSeekClient(),\n",
    "            \"mistral\": MistralClient()\n",
    "        }\n",
    "        \n",
    "        self.model_preferences = {\n",
    "            \"code_generation\": [\"deepseek\", \"claude\", \"openai\"],\n",
    "            \"code_review\": [\"gemini\", \"claude\", \"openai\"],\n",
    "            \"quick_tasks\": [\"gemini\", \"mistral\", \"openai\"],\n",
    "            \"complex_reasoning\": [\"claude\", \"openai\", \"gemini\"]\n",
    "        }\n",
    "    \n",
    "    def get_best_model(self, task_type: str, budget: str = \"medium\") -> str:\n",
    "        \"\"\"Get best model based on task and budget\"\"\"\n",
    "        budget_tiers = {\n",
    "            \"low\": [\"deepseek\", \"gemini\", \"mistral\"],\n",
    "            \"medium\": [\"openai\", \"claude\", \"gemini\"],\n",
    "            \"high\": [\"claude\", \"openai\", \"mistral\"]\n",
    "        }\n",
    "        \n",
    "        available_models = self.model_preferences.get(task_type, [\"openai\"])\n",
    "        budget_models = budget_tiers.get(budget, [\"openai\"])\n",
    "        \n",
    "        for model in available_models:\n",
    "            if model in budget_models:\n",
    "                return model\n",
    "        return available_models[0]\n",
    "    \n",
    "    def execute_task(self, task: str, task_type: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Execute task with appropriate model\"\"\"\n",
    "        model_key = self.get_best_model(task_type, kwargs.get('budget', 'medium'))\n",
    "        client = self.clients.get(model_key)\n",
    "        \n",
    "        if not client:\n",
    "            return {\"error\": True, \"message\": f\"Client {model_key} not found\"}\n",
    "        \n",
    "        try:\n",
    "            if task_type == \"code_generation\":\n",
    "                if hasattr(client, 'generate_code'):\n",
    "                    return client.generate_code(task, kwargs.get('language', 'python'))\n",
    "                elif hasattr(client, 'code_generation'):\n",
    "                    return client.code_generation(task, kwargs.get('language', 'python'))\n",
    "            elif task_type == \"code_review\":\n",
    "                if hasattr(client, 'code_analysis'):\n",
    "                    return client.code_analysis(task, kwargs.get('language', 'python'))\n",
    "                elif hasattr(client, 'code_review'):\n",
    "                    return client.code_review(task, kwargs.get('language', 'python'))\n",
    "            \n",
    "            # Fallback to generic chat\n",
    "            messages = [{\"role\": \"user\", \"content\": task}]\n",
    "            if hasattr(client, 'chat'):\n",
    "                return client.chat(messages)\n",
    "            elif hasattr(client, 'chat_completion'):\n",
    "                return client.chat_completion(messages)\n",
    "            elif hasattr(client, 'message'):\n",
    "                return client.message(task)\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\"error\": True, \"message\": str(e)}\n",
    "    \n",
    "    async def compare_models(self, prompt: str, models: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Compare responses from multiple models\"\"\"\n",
    "        if not models:\n",
    "            models = [\"openai\", \"gemini\", \"claude\", \"deepseek\"]\n",
    "            \n",
    "        tasks = []\n",
    "        for model_name in models:\n",
    "            client = self.clients.get(model_name)\n",
    "            if not client:\n",
    "                tasks.append(asyncio.sleep(0))  # Placeholder\n",
    "                continue\n",
    "                \n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            \n",
    "            if hasattr(client, 'async_chat_completion'):\n",
    "                task = client.async_chat_completion(messages)\n",
    "            elif hasattr(client, 'async_chat'):\n",
    "                task = client.async_chat(messages)\n",
    "            else:\n",
    "                # Fallback to sync wrapped in thread\n",
    "                if hasattr(client, 'chat'):\n",
    "                    task = asyncio.to_thread(client.chat, messages)\n",
    "                elif hasattr(client, 'chat_completion'):\n",
    "                    task = asyncio.to_thread(client.chat_completion, messages)\n",
    "                elif hasattr(client, 'message'):\n",
    "                    task = asyncio.to_thread(client.message, prompt)\n",
    "                else:\n",
    "                    task = asyncio.sleep(0)\n",
    "                    \n",
    "            tasks.append(task)\n",
    "            \n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        comparison = {}\n",
    "        for i, model_name in enumerate(models):\n",
    "            if isinstance(results[i], Exception):\n",
    "                comparison[model_name] = {\"error\": True, \"message\": str(results[i])}\n",
    "            else:\n",
    "                comparison[model_name] = results[i]\n",
    "                \n",
    "        return comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b3029a-309a-4ba8-a663-89a9d6d6a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 7. Error Handling & Utilities ====================\n",
    "class AIErrorHandler:\n",
    "    \"\"\"Comprehensive error handling for AI APIs\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def retry_on_error(func, max_retries: int = 3, delay: float = 1.0):\n",
    "        \"\"\"Decorator for retrying failed API calls\"\"\"\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        raise e\n",
    "                    print(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n",
    "                    time.sleep(delay * (2 ** attempt))  # Exponential backoff\n",
    "            return None\n",
    "        return wrapper\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_response(response: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Validate AI response quality\"\"\"\n",
    "        if response.get('error'):\n",
    "            return False\n",
    "            \n",
    "        content = response.get('content', '')\n",
    "        if not content or len(content.strip()) < 10:\n",
    "            return False\n",
    "            \n",
    "        # Check for common error patterns in AI responses\n",
    "        error_indicators = [\n",
    "            \"I cannot\", \"I'm unable\", \"as an AI\", \"I don't have\",\n",
    "            \"I'm not able\", \"I cannot provide\", \"I'm sorry\"\n",
    "        ]\n",
    "        \n",
    "        content_lower = content.lower()\n",
    "        return not any(indicator in content_lower for indicator in error_indicators)\n",
    "\n",
    "\n",
    "class CodeQualityChecker:\n",
    "    \"\"\"Utilities for checking generated code quality\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_python_syntax(code: str) -> bool:\n",
    "        \"\"\"Check if Python code is syntactically valid\"\"\"\n",
    "        try:\n",
    "            compile(code, '<string>', 'exec')\n",
    "            return True\n",
    "        except SyntaxError:\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_code_blocks(text: str, language: str = \"python\") -> List[str]:\n",
    "        \"\"\"Extract code blocks from AI responses\"\"\"\n",
    "        import re\n",
    "        pattern = f\"```{language}\\\\s*\\\\n(.*?)```\"\n",
    "        matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if not matches:\n",
    "            # Try without language specifier\n",
    "            pattern = r\"```(.*?)```\"\n",
    "            matches = re.findall(pattern, text, re.DOTALL)\n",
    "        return [match.strip() for match in matches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3dffce5-8f32-4875-b8cb-785890453365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OpenAI Example ===\n",
      "{'error': True, 'message': 'Rate limit exceeded', 'type': 'RateLimitError'}\n"
     ]
    }
   ],
   "source": [
    "# ==================== USAGE EXAMPLES ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: OpenAI\n",
    "    print(\"=== OpenAI Example ===\")\n",
    "    openai_client = OpenAIClient()\n",
    "    result = openai_client.code_generation(\n",
    "        \"Create a Python function to calculate fibonacci sequence with memoization\"\n",
    "    )\n",
    "    print(result.get(\"content\", result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c588fcd-b727-45a5-b6a6-050253891e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gemini Example ===\n",
      "{'error': True, 'message': '429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp\\nPlease retry in 54.767817026s. [links {\\n  description: \"Learn more about Gemini API quotas\"\\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n}\\n, violations {\\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\\n  quota_dimensions {\\n    key: \"model\"\\n    value: \"gemini-2.0-flash-exp\"\\n  }\\n  quota_dimensions {\\n    key: \"location\"\\n    value: \"global\"\\n  }\\n}\\nviolations {\\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\\n  quota_dimensions {\\n    key: \"model\"\\n    value: \"gemini-2.0-flash-exp\"\\n  }\\n  quota_dimensions {\\n    key: \"location\"\\n    value: \"global\"\\n  }\\n}\\n, retry_delay {\\n  seconds: 54\\n}\\n]', 'type': 'ResourceExhausted'}\n"
     ]
    }
   ],
   "source": [
    "# ==================== USAGE EXAMPLES ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 2: Gemini\n",
    "    print(\"\\n=== Gemini Example ===\")\n",
    "    gemini_client = GeminiClient()\n",
    "    analysis = gemini_client.code_analysis(\"\"\"\n",
    "def calculate_average(numbers):\n",
    "    return sum(numbers) / len(numbers)\n",
    "\"\"\")\n",
    "    print(analysis.get(\"content\", analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f21fd676-a30c-4415-8e17-bc909bd8fe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Claude Example ===\n",
      "{'error': True, 'message': \"Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}, 'request_id': 'req_011CViabYH3vCKXbGnfyvZFs'}\", 'type': 'BadRequestError'}\n"
     ]
    }
   ],
   "source": [
    "# ==================== USAGE EXAMPLES ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 3: Claude\n",
    "    print(\"\\n=== Claude Example ===\")\n",
    "    claude_client = ClaudeClient()\n",
    "    result = claude_client.programming_assistant(\n",
    "        \"Create a Python class for handling API requests with retry logic\"\n",
    "    )\n",
    "    print(result.get(\"content\", result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94433c7d-c032-4fa3-af6a-d819c2192ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DeepSeek Example ===\n",
      "{'error': True, 'message': \"Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\", 'type': 'APIStatusError'}\n"
     ]
    }
   ],
   "source": [
    "# ==================== USAGE EXAMPLES ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 4: DeepSeek\n",
    "    print(\"\\n=== DeepSeek Example ===\")\n",
    "    deepseek_client = DeepSeekClient()\n",
    "    code_result = deepseek_client.generate_code(\n",
    "        \"Create a FastAPI endpoint for user authentication with JWT tokens\"\n",
    "    )\n",
    "    print(code_result.get(\"content\", code_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18f5f1d3-a503-4be8-9643-b4feb3b67e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Manager Example ===\n",
      "{'error': True, 'message': \"Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}, 'request_id': 'req_011CViargBDyPjoUCFs716By'}\", 'type': 'BadRequestError'}\n"
     ]
    }
   ],
   "source": [
    "# ==================== USAGE EXAMPLES ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 5: Model Manager\n",
    "    print(\"\\n=== Model Manager Example ===\")\n",
    "    manager = AIModelManager()\n",
    "    result = manager.execute_task(\n",
    "        \"Create a Python data validation class using Pydantic\",\n",
    "        \"code_generation\",\n",
    "        budget=\"medium\"\n",
    "    )\n",
    "    print(result.get(\"content\", result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ad01e96-15c4-414a-9253-aae4baec4a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OpenAI Example ===\n",
      "{'error': True, 'message': 'Rate limit exceeded', 'type': 'RateLimitError'}\n",
      "\n",
      "=== Gemini Example ===\n",
      "{'error': True, 'message': '429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp\\nPlease retry in 39.78038924s. [links {\\n  description: \"Learn more about Gemini API quotas\"\\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n}\\n, violations {\\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\\n  quota_dimensions {\\n    key: \"model\"\\n    value: \"gemini-2.0-flash-exp\"\\n  }\\n  quota_dimensions {\\n    key: \"location\"\\n    value: \"global\"\\n  }\\n}\\nviolations {\\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\\n  quota_dimensions {\\n    key: \"model\"\\n    value: \"gemini-2.0-flash-exp\"\\n  }\\n  quota_dimensions {\\n    key: \"location\"\\n    value: \"global\"\\n  }\\n}\\n, retry_delay {\\n  seconds: 39\\n}\\n]', 'type': 'ResourceExhausted'}\n",
      "\n",
      "=== Claude Example ===\n",
      "{'error': True, 'message': \"Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}, 'request_id': 'req_011CViaBoCKPSeeG4G538jKt'}\", 'type': 'BadRequestError'}\n",
      "\n",
      "=== DeepSeek Example ===\n",
      "{'error': True, 'message': \"Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\", 'type': 'APIStatusError'}\n",
      "\n",
      "=== Model Manager Example ===\n",
      "{'error': True, 'message': \"Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}, 'request_id': 'req_011CViaCjQdtVkpHUZe2Ezup'}\", 'type': 'BadRequestError'}\n"
     ]
    }
   ],
   "source": [
    "# ==================== USAGE EXAMPLES ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: OpenAI\n",
    "    print(\"=== OpenAI Example ===\")\n",
    "    openai_client = OpenAIClient()\n",
    "    result = openai_client.code_generation(\n",
    "        \"Create a Python function to calculate fibonacci sequence with memoization\"\n",
    "    )\n",
    "    print(result.get(\"content\", result))\n",
    "    \n",
    "    # Example 2: Gemini\n",
    "    print(\"\\n=== Gemini Example ===\")\n",
    "    gemini_client = GeminiClient()\n",
    "    analysis = gemini_client.code_analysis(\"\"\"\n",
    "def calculate_average(numbers):\n",
    "    return sum(numbers) / len(numbers)\n",
    "\"\"\")\n",
    "    print(analysis.get(\"content\", analysis))\n",
    "    \n",
    "    # Example 3: Claude\n",
    "    print(\"\\n=== Claude Example ===\")\n",
    "    claude_client = ClaudeClient()\n",
    "    result = claude_client.programming_assistant(\n",
    "        \"Create a Python class for handling API requests with retry logic\"\n",
    "    )\n",
    "    print(result.get(\"content\", result))\n",
    "    \n",
    "    # Example 4: DeepSeek\n",
    "    print(\"\\n=== DeepSeek Example ===\")\n",
    "    deepseek_client = DeepSeekClient()\n",
    "    code_result = deepseek_client.generate_code(\n",
    "        \"Create a FastAPI endpoint for user authentication with JWT tokens\"\n",
    "    )\n",
    "    print(code_result.get(\"content\", code_result))\n",
    "    \n",
    "    # Example 5: Model Manager\n",
    "    print(\"\\n=== Model Manager Example ===\")\n",
    "    manager = AIModelManager()\n",
    "    result = manager.execute_task(\n",
    "        \"Create a Python data validation class using Pydantic\",\n",
    "        \"code_generation\",\n",
    "        budget=\"medium\"\n",
    "    )\n",
    "    print(result.get(\"content\", result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb9d6db-1a5a-411f-a36a-64f21a24a570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
