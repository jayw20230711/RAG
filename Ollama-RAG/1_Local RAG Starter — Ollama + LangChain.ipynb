{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2496eb5-a8fc-4444-a328-bd773c0ea46b",
   "metadata": {},
   "source": [
    "#-----------\n",
    "# Install Dependencies\n",
    "#-----------\n",
    "!pip install langchain langchain-community faiss-cpu sentence-transformers transformers torch python-dotenv tqdm requests\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca3a2455-9e49-4394-bfbb-d591155e71e2",
   "metadata": {},
   "source": [
    "#-----------\n",
    "# Create a Sample Document\n",
    "#-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c225be-2ac6-4bbd-a3b7-1804d74512cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.txt created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "sample_text = \"\"\"\n",
    "LangChain + Olllama RAG Test Document\n",
    "\n",
    "This document discusses embeddings , retrieval,  LangChain text splitters, and running local LLMs such as phi3, mistral, and llama3.1 via Olama.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"data/sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    if f.write(sample_text):\n",
    "        print(\"sample.txt created!\")\n",
    "    else: print(\"write failed!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1705249-d729-4f8a-9abe-5d529b012e8f",
   "metadata": {},
   "source": [
    "#-----------\n",
    "# Load + Chunk Text\n",
    "#-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025fea44-d815-48d5-8e7c-a2d19d4fc03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: [Document(metadata={'source': 'data\\\\sample.txt'}, page_content='\\nLangChain + Olllama RAG Test Document\\n\\nThis document discusses embeddings , retrieval,  LangChain text splitters, and running local LLMs such as phi3, mistral, and llama3.1 via Olama.\\n')]\n"
     ]
    }
   ],
   "source": [
    "# Simple way to load a file\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "\n",
    "\n",
    "loader = DirectoryLoader(\"data/\", glob=\"*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "print(\"Documents:\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca233e77-2c98-4e3e-9056-002326e42063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks: 1\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(\"chunks:\", len(chunks))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09ddbde5-0ba5-420d-9bf9-588154770324",
   "metadata": {},
   "source": [
    "#-----------\n",
    "# Build Vector Index (FAISS + Embeddings)\n",
    "#-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b564c045-961a-4c1a-8787-e18e5b6dcbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved.\n"
     ]
    }
   ],
   "source": [
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "faiss_index = FAISS.from_documents(chunks, embeddings)\n",
    "faiss_index.save_local(\"faiss_index\")\n",
    "\n",
    "#retriever = faiss_index.as_retriever()\n",
    "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "\n",
    "print(\"FAISS index saved.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "080742c7-dd74-49e7-b4f7-5c812601a385",
   "metadata": {},
   "source": [
    "#-----------\n",
    "# Load the Index for Retrieval\n",
    "#-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e3fabf-045b-4ab4-8c31-78e14fde9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.llms import Ollama\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = OllamaLLM(model=\"phi3\")               # you can use mistral, llama3.1\n",
    "#llm = Ollama(modle=\"mistral\")\n",
    "#llm = Ollama(model=\"llama3.1\")\n",
    "# llm = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a205a0-8999-44ed-9b83-716c66efd405",
   "metadata": {},
   "source": [
    "#-----------\n",
    "# Create the RAG Chain (LCEL)\n",
    "#-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7252ab8-f3ee-4ea1-8e34-63412ef97ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "#from langchain_community.llms import Ollama\n",
    "from langchain_core.runnables import RunnableMap, RunnableSequence, RunnableLambda\n",
    "#from langchain_core.outputs import StringOutputParser   - depricated\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952aaab1-8926-444c-b3ce-fc28247251c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "\"Use the following context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c715dd50-813b-4158-9990-5876b16e18f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain(retriever):\n",
    "    return(\n",
    "        {\n",
    "            \"context\": lambda x: \"\\n\\n\".join(\n",
    "                doc.page_content for doc in retriever.invoke(x[\"question\"])\n",
    "            ),\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2bd98-4767-4c56-be58-228106f44583",
   "metadata": {},
   "source": [
    "#-----------\n",
    "# Build the chain\n",
    "#-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a79459-a431-4645-90a9-37a20f8c9074",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = rag_chain(retriever)\n",
    "#chain"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c446a77c-4951-4693-9600-bc05c76f48cc",
   "metadata": {},
   "source": [
    "# Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca2c65b9-d22c-4779-8540-c90f018f7781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document highlights that the embeddings are based on Olllama RAG (Retrieval-Augmented Generation) technology for efficient text processing with large language models, using tools like Olama. It also mentions various aspects of LangChain such as its role in splitting texts and facilitating local runs of LLMs including phi3, mistral, and llama3.1 through Olala interface on a user's device or server without the need for internet connectivity during inference time. The aim is to improve speed by offloading computation from cloud infrastructure where possible. Moreover, it talks about using tools like LangChain text splitters in conjunction with Olllama and how they can help optimize large language model computations locally on smaller hardware setups or even mobile devices when internet access may not be available during inference time. This document seems to emphasize the potential of these technologies for efficient local processing, especially where connectivity is a concern while running high-capacity LLMs like phi3, mistral and llama3.1 using tools such as Olala or locally installed versions on various hardware setups including mobile devices without relying heavily on cloud infrastructure during the inference phase of tasks.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"What does this document say about LangChain?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e6baf50-2df4-4930-991d-12e7e96be060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the original text \"This document discusses embeddings , retrieval,  LangChain text splitters, and running local LLMs such as phi3, mistral, and llama3.1 via Olama.\" You can run the Phi-Phi (phi3), Mistral, and Llama models locally using Ollama platform. These include three versions of large language models like LLama 3.1 specifically designed to work on local machines with low hardware requirements compared to cloud-based services.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"Which models can I run locally with Ollama?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "596def14-2837-4833-ae70-2f66d9ef8106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ChatGeneration', 'ChatGenerationChunk', 'ChatResult', 'Generation', 'GenerationChunk', 'LLMResult', 'RunInfo']\n"
     ]
    }
   ],
   "source": [
    "import langchain_core.outputs\n",
    "print(dir(langchain_core.outputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
